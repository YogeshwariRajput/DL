{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ab861a-50f4-41f9-8ff8-d0807d5171fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASSIGNMENT 5\n",
    "#Title:  Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "#a. Data preparation\n",
    "#b. Generate training data\n",
    "#c. Train model\n",
    "#d. Output\n",
    "#Name:PRANJAL SUHAS PATIL\n",
    "#Roll No:46\n",
    "#Batch: IT-3\n",
    "#Department: Information Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff83fb60-d88b-43b5-8c35-a096725543ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f477a7-892c-4ae2-a219-6e05ad898c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# (a) DATA PREPARATION\n",
    "# -----------------------------------------\n",
    "text = \"we are studying deep learning with simple examples to understand how models work\"\n",
    "\n",
    "words = text.lower().split()   # convert text to lowercase and split into individual words\n",
    "\n",
    "vocab = list(set(words))   # create a list of unique words (vocabulary)\n",
    "vocab_size = len(vocab)     # number of unique words\n",
    "embed_dim = 8               #Size of each word vector (embedding).Each word will be represented by an 8-dimensional vector.\n",
    "window = 2                  #Window = 2 means:Take 2 words before the target wordTake 2 words after the target word\n",
    "\n",
    "word_to_idx = {w:i for i,w in enumerate(vocab)}      # dictionary: word â†’ index number\n",
    "idx_to_word = {i:w for w,i in word_to_idx.items()}    #idx_to_word = {i:w for w, i in word_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97755673-eea6-4cd2-89e0-c66eefe76adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# (b) GENERATE TRAINING DATA (CBOW)\n",
    "# -----------------------------------------\n",
    "data = []        # list to store all (context, target) pairs\n",
    "\n",
    "for i in range(window, len(words)-window):        # loop through each word, skipping first 2 and last 2 words\n",
    "    \n",
    "    context = [words[i-2], words[i-1], words[i+1], words[i+2]]   # pick 4 context words (2 before and 2 after the target word)\n",
    "    target = words[i]         # the middle word is the target word\n",
    "    data.append((context, target))        # add the pair (context words, target word) to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5632877a-4936-4230-b1c1-ec4ec473aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# (c) TRAIN MODEL\n",
    "# -----------------------------------------\n",
    "emb = np.random.randn(vocab_size, embed_dim)\n",
    "W = np.random.randn(embed_dim*4, vocab_size)\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for ctx, tgt in data:\n",
    "        idxs = [word_to_idx[w] for w in ctx]\n",
    "        tgt_idx = word_to_idx[tgt]\n",
    "\n",
    "        x = emb[idxs].reshape(-1)\n",
    "        y = softmax(x @ W)\n",
    "\n",
    "        y[tgt_idx] -= 1\n",
    "        W -= 0.05 * np.outer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b0235b4-80ad-4101-ad79-a77bbe54f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------\n",
    "# (d) OUTPUT\n",
    "# -----------------------------------------\n",
    "def predict(context):\n",
    "    idxs = [word_to_idx[w] for w in context]\n",
    "    x = emb[idxs].reshape(-1)\n",
    "    y = softmax(x @ W)\n",
    "    return idx_to_word[np.argmax(y)]\n",
    "\n",
    "# print(\"Prediction:\", predict([\"we\", \"are\", \"to\", \"study\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9a6553c-733e-4899-be0d-d0ba235ccd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: studying\n",
      "Prediction 2: deep\n",
      "Prediction 3: learning\n",
      "Prediction 4: with\n",
      "Prediction 5: simple\n",
      "Prediction 6: examples\n",
      "Prediction 7: to\n",
      "Prediction 8: understand\n",
      "Prediction 9: how\n",
      "Prediction 10: to\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction 1:\", predict([\"we\", \"are\", \"deep\", \"learning\"]))\n",
    "print(\"Prediction 2:\", predict([\"are\", \"studying\", \"learning\", \"with\"]))\n",
    "print(\"Prediction 3:\", predict([\"studying\", \"deep\", \"with\", \"simple\"]))\n",
    "print(\"Prediction 4:\", predict([\"deep\", \"learning\", \"simple\", \"examples\"]))\n",
    "print(\"Prediction 5:\", predict([\"learning\", \"with\", \"examples\", \"to\"]))\n",
    "print(\"Prediction 6:\", predict([\"with\", \"simple\", \"to\", \"understand\"]))\n",
    "print(\"Prediction 7:\", predict([\"simple\", \"examples\", \"understand\", \"how\"]))\n",
    "print(\"Prediction 8:\", predict([\"examples\", \"to\", \"how\", \"models\"]))\n",
    "print(\"Prediction 9:\", predict([\"to\", \"understand\", \"models\", \"work\"]))\n",
    "print(\"Prediction 10:\", predict([\"understand\", \"how\", \"work\", \"we\"]))  # wraps around\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8a807-ba42-4f37-b643-8da15ba56ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dee5425e-b2ae-4be7-a1de-25487faa5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: about\n",
      "Prediction 9: the\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # (a) DATA PREPARATION\n",
    "# # --------------------------------------------------\n",
    "# text =\"\"\"We are about to study the idea of a computational process.\n",
    "# Computational processes are abstract beings that inhabit computers.\n",
    "# As they evolve, processes manipulate other abstract things called data.\n",
    "# The evolution of a process is directed by a pattern of rules\n",
    "# called a program. People create programs to direct processes. In effect,\n",
    "# we conjure the spirits of the computer with our spells.\"\"\"\n",
    "# text = text.lower()\n",
    "# words = text.split()\n",
    "\n",
    "# vocab = list(set(words))\n",
    "# vocab_size = len(vocab)\n",
    "# embed_dim = 8\n",
    "# window = 2\n",
    "\n",
    "# word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "# idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # (b) GENERATE TRAINING DATA (CBOW)\n",
    "# # --------------------------------------------------\n",
    "# data = []\n",
    "# for i in range(window, len(words)-window):\n",
    "#     context = [words[i-2], words[i-1], words[i+1], words[i+2]]\n",
    "#     target = words[i]\n",
    "#     data.append((context, target))\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # (c) TRAIN MODEL (VERY SIMPLE CBOW)\n",
    "# # --------------------------------------------------\n",
    "# emb = np.random.randn(vocab_size, embed_dim)\n",
    "# W = np.random.randn(embed_dim*4, vocab_size)\n",
    "\n",
    "# def softmax(x):\n",
    "#     e = np.exp(x - np.max(x))\n",
    "#     return e / e.sum()\n",
    "\n",
    "# for epoch in range(25):\n",
    "#     for ctx, tgt in data:\n",
    "\n",
    "#         ctx_idx = [word_to_idx[w] for w in ctx]\n",
    "#         tgt_idx = word_to_idx[tgt]\n",
    "\n",
    "#         ctx_vec = emb[ctx_idx].reshape(-1)\n",
    "#         scores = ctx_vec @ W\n",
    "#         probs = softmax(scores)\n",
    "\n",
    "#         probs[tgt_idx] -= 1\n",
    "#         W -= 0.05 * np.outer(ctx_vec, probs)\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # (d) OUTPUT (Prediction)\n",
    "# # --------------------------------------------------\n",
    "# def predict(context_words):\n",
    "#     idxs = [word_to_idx[w] for w in context_words]\n",
    "#     vec = emb[idxs].reshape(-1)\n",
    "#     probs = softmax(vec @ W)\n",
    "#     return idx_to_word[np.argmax(probs)]\n",
    "\n",
    "# print(\"Prediction:\", predict([\"we\", \"are\", \"to\", \"study\"]))\n",
    "# print(\"Prediction 9:\", predict([\"spirits\",\"of\",\"computer\",\"with\"]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb606b8-5905-4613-a620-5e1375573116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961728b-adac-4943-8284-723076c3f7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01079ab5-b6bd-4f12-881b-8c15fa86cae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
